%feature("docstring") OT::GeneralizedLinearModelAlgorithm
"Algorithm for the evaluation of general linear models.

Available constructors:
    GeneralizedLinearModelAlgorithm(*inputSample, outputSample, covarianceModel, basis, normalize=True*)

    GeneralizedLinearModelAlgorithm(*inputSample, inputTransformation, outputSample, covarianceModel, basis*)

    GeneralizedLinearModelAlgorithm(*inputSample, outputSample, covarianceModel, multivariateBasis, normalize=True*)

    GeneralizedLinearModelAlgorithm(*inputSample, inputTransformation, outputSample, covarianceModel, multivariateBasis*)

Parameters
----------
inputSample, outputSample : :class:`~openturns.NumericalSample` or 2d-array
    The input and output samples of a model evaluated apart.

inputTransformation : :class:`~openturns.NumericalMathFunction`
    Function that helps to normalize the input sample.

basis : :class:`~openturns.Basis`
    Functional basis to estimate the trend.
    If the output dimension is greater than 1, the same basis is used for all marginals.

multivariateBasis : collection of :class:`~openturns.Basis`
    Collection of functional basis: one basis for each marginal output.
    If the trend is not estimated, the collection must be empty.

covarianceModel : :class:`~openturns.CovarianceModel`
    Covariance model.
    Should have spatial dimension equal to input sample's dimension and dimension equal to output sample's dimension.
    See note for some particular applications.

normalize : bool, optional
    Indicates whether the input sample has to be normalized.

    OpenTURNS uses the transformation fixed by the User in *inputTransformation* or the empirical mean and variance of the input sample.
    Default is *True*.

keepCovariance : bool, optional
    Indicates whether the covariance matrix has to be stored in the result structure *GeneralizedLinearModelResult*.

    Default is *False*.

Notes
-----
We suppose we have an input sample :math:`(\vect{x}_1, \dots, \vect{x}_m)` associated to the output sample :math:`(\vect{y}_1, \dots, \vect{y}_m)` where :math:`\vect{y}_i = \cM(\vect{x}_i)` for all *i*,
:math:`\cM:\Rset^d \mapsto \Rset^p`

The objective is to build a metamodel :math:`\tilde{\cM}`, using a **generalized linear model** of :math:`\vect{y}` as follows:

.. math::
    \tilde{\cM}(\vect{x}) = \vect{F}^t(\vect{x}) \vect{\beta} + \vect{Z}

with :math:`\mat{F} \in \mathcal{M}_{np, M}(\Rset)`:

.. math::
    \mat{F}(\vect{x}) = \left(
      \begin{array}{lcl}
        \vect{f}_1(\vect{x}_1) & \dots & \vect{f}_M(\vect{x}_1) \\
        \dots & \dots & \\
        \vect{f}_1(\vect{x}_n) & \dots & \vect{f}_M(\vect{x}_n)
       \end{array}
     \right)

:math:`(f_1, \dots, f_M)` is a functional basis whith :math:`f_i: \Rset^d \mapsto \Rset^p` for all *i*, :math:`\beta` are the coefficients of the linear combination and :math:`Z` is a zero-mean gaussian process with a stationary covariance function :math:`C_{\vect{\sigma}, \vect{\theta}, \vect{\lambda}}: \Rset^d \times \Rset^d \mapsto \mathcal{S}^{+}_{p}(\Rset)`:

.. math::
    C_{\vect{\sigma}, \vect{\theta}, \vect{\lambda}}(\vect{x}, \vect{y}) = \Expect{Z(\omega, \vect{x})Z(\omega, \vect{y})}

where :math:`\vect{\sigma} \in \Rset^p` is the amplitude vector, :math:`\vect{\theta} \in \Rset^d` the scale vector and :math:`\vect{\lambda}` a set of residual parameters.

We write :math:`C_{\vect{\sigma}, \vect{\theta}, \vect{\lambda}}(\vect{\tau})` rather than :math:`C_{\vect{\sigma}, \vect{\theta}, \vect{\lambda}}(\vect{x},\vect{x}+\vect{\tau})` due to stationarity.

If :math:`R_{\vect{\theta}, \vect{\lambda}}` is the stationary correlation function, then we have:

.. math::
    C_{\vect{\sigma}, \vect{\theta}, \vect{\lambda}}(\vect{\tau}) = diag(\vect{\sigma})R_{\vect{\theta}, \vect{\lambda}}(\vect{\tau})  diag(\vect{\sigma}), \
    R_{\vect{\theta}, \vect{\lambda}}(\vect{\tau}) = \rho_{\vect{\lambda}}(\tau_1/\theta_1, \dots)

OpenTURNS estimates with the maximum likelihood method the following parameters:

    - :math:`(\beta_1, \dots, \beta_M)`,
    - :math:`\sigma_1, \dots, \sigma_p`,
    - :math:`\theta_1= \dots, \theta_d`

Default optimizer is :class:`~openturns.TNC` and might be changed thanks to the *setOptimizationSolver* method. User could also change the default optimization solver by setting the `GeneralizedLinearModelAlgorithm-DefaultOptimizationSolver` resource map key to `NELDER-MEAD` or `LBFGS` respectively for Nelder-Mead and LBFGS-B solvers.

It is also possible to proceed as follows:

    - ask for the log-likelihood function of the *GeneralizedLinearModelAlgorithm* thanks to the *getObjectiveFunction()* method
    - optimize it with respect to the parameters :math:`\vect{\theta}` and  :math:`\vect{\sigma}` using any optimisation algorithms (that can take into account some additional constraints if needed)
    - fulfill the *GeneralizedLinearModelAlgorithm* with the optimized value of these parameters.

The *GeneralizedLinearModelAlgorithm* class provides methods to build such a meta model using different input arguments. With huge data, the `hierarchical matrix <http://en.wikipedia.org/wiki/Hierarchical_matrix>`_  implementation could be used if OpenTURNS had been compiled with `hmat-oss` support.

This implementation, which is based on a sparse representation of an approximated covariance matrix (and its Cholesky factor), has a better complexity both in terms of memory requirements and floating point operations. To use it, the `GeneralizedLinearModelAlgorithm-LinearAlgebra` resource map key should be instancied to `HMAT`. Default value of the key is `LAPACK`.

Note also that covariance model's dimensions should be equal to input/output sample's dimension. Otherwise:

   - If the covariance model fixed by the User has a spatial dimension (*d*)  equal to 1 with parameters :math:`(\sigma, \theta, \vect{\lambda})` whereas the input sample dimension has a greater dimension, then OpenTURNS builds a product model covariance (see :class:`~openturns.ProductCovarianceModel`) that writes :math:`C_{\sigma, \theta, \vect{\lambda}}(\vect{\tau}) = \prod_{i=1}^d C_{\sigma, \theta, \vect{\lambda}}(\tau_i)`

   - If output dimension (*p*) is greater than one, it is possible to build a tensorized covariance model thanks to the :class:`~openturns.TensorizedCovarianceModel`


Examples
--------
Create the model :math:`\cM: \Rset \mapsto \Rset` and the samples:

>>> import openturns as ot
>>> f = ot.NumericalMathFunction(['x'], ['x * sin(x)'])
>>> inputSample = ot.NumericalSample([[1.], [3.], [5.], [6.,], [7.], [8.]])
>>> outputSample = f(inputSample)

Create the algorithm:

>>> basis = ot.ConstantBasisFactory().build()
>>> covarianceModel = ot.SquaredExponential(1)
>>> algo = ot.GeneralizedLinearModelAlgorithm(inputSample, outputSample, covarianceModel, basis)
>>> algo.run()

Get the resulting meta model:

>>> result = algo.getResult()
>>> metamodel = result.getMetaModel()"

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getInputTransformation
"Get the function normalizing the input.

Returns
-------
transformation : :class:`~openturns.NumericalMathFunction`
    Function that normalizes the input."

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::setInputTransformation
"Set the function normalizing the input.

Parameters
----------
transformation : :class:`~openturns.NumericalMathFunction`
    Function that normalizes the input.
    The input dimension should be the same as input's sample dimension, output dimension
    should be output sample's dimension"

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getResult
"Get the results of the metamodel computation.

Returns
-------
result : :class:`~openturns.GeneralizedLinearModelResult`
    Structure containing all the results obtained after computation
    and created by the method :py:meth:`run`.

Examples
--------
Create the model :math:`\cM: \Rset \mapsto \Rset` and the samples:

>>> import openturns as ot
>>> f = ot.NumericalMathFunction(['x'], ['x * sin(x)'])
>>> inputSample = ot.NumericalSample([[1.], [3.], [5.], [6.,], [7.], [8.]])
>>> outputSample = f(inputSample)

Create the algorithm:

>>> basis = ot.ConstantBasisFactory().build()
>>> covarianceModel = ot.SquaredExponential(1)
>>> algo = ot.GeneralizedLinearModelAlgorithm(inputSample, outputSample, covarianceModel, basis)
>>> algo.run()

Get the result:

>>> result = algo.getResult()
"

//-----------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getInputSample
"Accessor to the input sample.

Returns
-------
inputSample : :class:`~openturns.NumericalSample`
    The input sample."

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getOutputSample
"Accessor to the output sample.

Returns
-------
outputSample : :class:`~openturns.NumericalSample`
    The output sample."

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getObjectiveFunction()
"Accessor to the log-likelihood function that writes as argument of the covariance's model parameters.

Returns
-------
logLikelihood : :class:`~openturns.NumericalMathFunction`
    The log-likelihood function as a fucntion of the covariance's model parameters.

Notes
-----
The log-likelihood function may be useful for some postprocessing: maximization using external optimizers for example.


Examples
--------
Create the model :math:`\cM: \Rset \mapsto \Rset` and the samples:

>>> import openturns as ot
>>> f = ot.NumericalMathFunction(['x0'], ['f0'], ['x0 * sin(x0)'])
>>> inputSample = ot.NumericalSample([[1.], [3.], [5.], [6.,], [7.], [8.]])
>>> outputSample = f(inputSample)

Create the algorithm:

>>> basis = ot.ConstantBasisFactory().build()
>>> covarianceModel = ot.SquaredExponential(1)
>>> algo = ot.GeneralizedLinearModelAlgorithm(inputSample, outputSample, covarianceModel, basis)
>>> algo.run()

Get the log-likelihood function:

>>> likelihoodFunction = algo.getObjectiveFunction()
"


// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::run
"Compute the response surface.

Notes
-----
It computes the response surface and creates a
:class:`~openturns.GeneralizedLinearModelResult` structure containing all the results."


// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::enableKeepCovariance
"Enable the mechanism of storing the covariance Cholesky factor"

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::disableKeepCovariance
"Disable the mechanism of storing the covariance Cholesky factor"

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::isEnabledKeepCovariance
"Test whether the store mechanism of covariance Cholesky factor is enabled or not.

Returns
-------
isEnabled : bool
    Flag telling whether the store mechanism of covariance is enabled.
    It is disabled by default."

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::getOptimizationSolver
"Accessor to solver used to optimize the covariance model parameters.

Returns
-------
algorithm : :class:`~openturns.OptimizationSolver`
    Solver used to optimize the covariance model parameters.
    Default optimizer is :class:`~openturns.TNC`"

// ---------------------------------------------------------------------

%feature("docstring") OT::GeneralizedLinearModelAlgorithm::setOptimizationSolver
"Accessor to the solver used to optimize the covariance model parameters.

Parameters
----------
algorithm : :class:`~openturns.OptimizationSolver`
    Solver used to optimize the covariance model parameters.

"
// ---------------------------------------------------------------------
